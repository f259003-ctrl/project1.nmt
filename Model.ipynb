{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0c1wk4dvSWe",
        "outputId": "b764ffd6-4c0a-4be1-d92f-2596f3fb4e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 04:50:35--  https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/amir9ume/urdu_ghazals_rekhta/main/dataset/dataset.zip [following]\n",
            "--2025-09-22 04:50:36--  https://raw.githubusercontent.com/amir9ume/urdu_ghazals_rekhta/main/dataset/dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2927519 (2.8M) [application/zip]\n",
            "Saving to: ‚Äòdataset.zip‚Äô\n",
            "\n",
            "dataset.zip         100%[===================>]   2.79M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-09-22 04:50:36 (33.6 MB/s) - ‚Äòdataset.zip‚Äô saved [2927519/2927519]\n",
            "\n",
            "‚úÖ Dataset extracted at: urdu_dataset\n",
            "üìä Urdu files: 1314\n",
            "üìä Roman files: 1314\n",
            "‚úÖ Total pairs: 21003\n",
            "‚úÖ Cleaned sample:\n",
            "                                  Urdu Text  \\\n",
            "6790          ÿ™ŸÖÿßŸÖ ÿ≠ÿ≥ŸÜ ⁄©€í ÿ¨ŸÑŸà€í ÿ™ŸÖÿßŸÖ ŸÖÿ≠ÿ±ŸàŸÖ€å   \n",
            "10823  ÿßÿ® ÿØŸÑ ⁄©Ÿà ÿÆÿØÿß ÿ±⁄©⁄æ€í ÿßÿ® ÿØŸÑ ⁄©ÿß ÿ≤ŸÖÿßŸÜÿß €Å€í   \n",
            "4680     ÿßÿ®⁄æ€å ÿ≥ŸÜ €Å€å ⁄©€åÿß €Å€í ÿ¨Ÿà ÿ®€åÿ®ÿß⁄©€åÿß⁄∫ €ÅŸà⁄∫   \n",
            "\n",
            "                             Roman Transliteration  \n",
            "6790             tamƒÅm husn ke jalve tamƒÅm mahr≈´mƒ´  \n",
            "10823  ab dil ko ·∏≥hudƒÅ rakkhe ab dil kƒÅ zamƒÅnƒÅ hai  \n",
            "4680          abhƒ´ sin hƒ´ kyƒÅ hai jo bebƒÅkiyƒÅ√± ho√±  \n",
            "üìÇ Tokenized sample:\n",
            "                                               Urdu Text  \\\n",
            "2239  ÿ™Ÿà Ÿæ€åÿ± ŸÖ€í ÿÆÿßŸÜ€Å ÿ≥ŸÜ ⁄©€í ⁄©€ÅŸÜ€í ŸÑ⁄Øÿß ⁄©€Å ŸÖŸÜ€Å Ÿæ⁄æŸπ €Å€í ÿÆÿß...   \n",
            "5097                       ÿØŸÖ ÿπÿ™ÿßÿ® ÿ¨Ÿà ÿ±ŸÜ⁄Øÿ™ ÿ™ÿ±€å ŸÜ⁄©ŸÑÿ™€å €Å€í   \n",
            "6384                   ⁄©€Åÿ™ÿß €ÅŸà⁄∫ ÿßÿ≥€í ŸÖ€å⁄∫ ÿ™Ÿà ÿÆÿµŸàÿµ€åÿ™ ŸæŸÜ€Åÿß⁄∫   \n",
            "\n",
            "                                  Roman Transliteration  \\\n",
            "2239  to piir mai·∏≥hƒÅna sun ke kahne lagƒÅ ki mu√±h pha...   \n",
            "5097                dameitƒÅb jo ra√±gat tirƒ´ nikaltƒ´ hai   \n",
            "6384          kahtƒÅ huu√± use mai√± to ·∏≥hus≈´siyyatepinhƒÅ√±   \n",
            "\n",
            "                                            Urdu Tokens  \\\n",
            "2239  ÿ™ Ÿà   Ÿæ €å ÿ±   ŸÖ €í   ÿÆ ÿß ŸÜ €Å   ÿ≥ ŸÜ   ⁄© €í   ⁄© €Å ...   \n",
            "5097  ÿØ ŸÖ   ÿπ ÿ™ ÿß ÿ®   ÿ¨ Ÿà   ÿ± ŸÜ ⁄Ø ÿ™   ÿ™ ÿ± €å   ŸÜ ⁄© ŸÑ ...   \n",
            "6384  ⁄© €Å ÿ™ ÿß   €Å Ÿà ⁄∫   ÿß ÿ≥ €í   ŸÖ €å ⁄∫   ÿ™ Ÿà   ÿÆ ÿµ Ÿà ...   \n",
            "\n",
            "                                           Roman Tokens  \n",
            "2239  t o   p i i r   m a i ·∏≥ h ƒÅ n a   s u n   k e ...  \n",
            "5097  d a m e i t ƒÅ b   j o   r a √± g a t   t i r ƒ´ ...  \n",
            "6384  k a h t ƒÅ   h u u √±   u s e   m a i √±   t o   ...  \n",
            "‚úÖ Split sizes:\n",
            "Train: 10501\n",
            "Validation: 5251\n",
            "Test: 5251\n",
            "üìÇ CSV files saved: train_data.csv, valid_data.csv, test_data.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import string\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Download & Extract Dataset\n",
        "# -------------------------------\n",
        "dataset_url = \"https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\"\n",
        "!wget -O dataset.zip \"$dataset_url\"\n",
        "\n",
        "unzip_dir = \"urdu_dataset\"\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n",
        "\n",
        "print(\"‚úÖ Dataset extracted at:\", unzip_dir)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Collect Files (Urdu + Roman)\n",
        "# -------------------------------\n",
        "urdu_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"ur\", \"*\"), recursive=True))\n",
        "roman_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"en\", \"*\"), recursive=True))\n",
        "\n",
        "print(\"üìä Urdu files:\", len(urdu_files))\n",
        "print(\"üìä Roman files:\", len(roman_files))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Create Urdu-Roman Pairs\n",
        "# -------------------------------\n",
        "pairs = []\n",
        "for ur_path, ro_path in zip(urdu_files, roman_files):\n",
        "    with open(ur_path, \"r\", encoding=\"utf-8\") as ur, open(ro_path, \"r\", encoding=\"utf-8\") as ro:\n",
        "        ur_lines, ro_lines = ur.readlines(), ro.readlines()\n",
        "        for u, r in zip(ur_lines, ro_lines):\n",
        "            u, r = u.strip(), r.strip()\n",
        "            if u and r:\n",
        "                pairs.append((u, r))\n",
        "\n",
        "df = pd.DataFrame(pairs, columns=[\"Urdu Text\", \"Roman Transliteration\"])\n",
        "print(\"‚úÖ Total pairs:\", len(df))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Clean Urdu & Roman Text\n",
        "# -------------------------------\n",
        "def clean_urdu(text):\n",
        "    text = re.sub(r'[ŸéŸãŸèŸåŸêŸçŸíŸëŸ∞]', '', text)  # remove diacritics\n",
        "    text = text.replace(\"ÿ£\", \"ÿß\").replace(\"ÿ•\", \"ÿß\").replace(\"ÿ¢\", \"ÿß\")  # normalize alef\n",
        "    text = text.replace(\"Ÿä\", \"€å\").replace(\"ŸÉ\", \"⁄©\")  # normalize Yeh, Kaaf\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # keep only Urdu letters\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def clean_roman(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df[\"Urdu Text\"] = df[\"Urdu Text\"].apply(clean_urdu)\n",
        "df[\"Roman Transliteration\"] = df[\"Roman Transliteration\"].apply(clean_roman)\n",
        "\n",
        "print(\"‚úÖ Cleaned sample:\\n\", df.sample(3))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Tokenization (character-level for transliteration)\n",
        "# -------------------------------\n",
        "def char_tokenize(text):\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "df[\"Urdu Tokens\"] = df[\"Urdu Text\"].apply(char_tokenize)\n",
        "df[\"Roman Tokens\"] = df[\"Roman Transliteration\"].apply(char_tokenize)\n",
        "\n",
        "print(\"üìÇ Tokenized sample:\\n\", df.sample(3))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Train/Validation/Test Split (50/25/25)\n",
        "# -------------------------------\n",
        "train_data, temp_data = train_test_split(df, test_size=0.50, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.50, random_state=42)\n",
        "\n",
        "print(\"‚úÖ Split sizes:\")\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(valid_data))\n",
        "print(\"Test:\", len(test_data))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 7: Save Splits to CSV\n",
        "# -------------------------------\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "valid_data.to_csv(\"valid_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "print(\"üìÇ CSV files saved: train_data.csv, valid_data.csv, test_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline: dataset -> preprocess -> char vocab -> Seq2Seq (BiLSTM enc, LSTM dec) -> train (5 epochs) -> eval metrics\n",
        "# Save as run_transliteration_experiments.py and run, or paste into a notebook cell.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import glob\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "def download_and_extract():\n",
        "    dataset_url = \"https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\"\n",
        "    if not os.path.exists(\"dataset.zip\"):\n",
        "        print(\"Downloading dataset.zip ...\")\n",
        "        # If running in a shell-enabled environment like Colab, uncomment:\n",
        "        # !wget -O dataset.zip \"$dataset_url\"\n",
        "        # If wget unavailable, the user should provide dataset.zip manually.\n",
        "        raise RuntimeError(\"Please download dataset.zip manually into working directory or enable shell commands.\")\n",
        "    print(\"Extracting dataset.zip ...\")\n",
        "    unzip_dir = \"urdu_dataset\"\n",
        "    os.makedirs(unzip_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)\n",
        "    return unzip_dir\n",
        "\n",
        "def collect_pairs(unzip_dir):\n",
        "    urdu_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"ur\", \"*\"), recursive=True))\n",
        "    roman_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"en\", \"*\"), recursive=True))\n",
        "    pairs = []\n",
        "    for u_path, r_path in zip(urdu_files, roman_files):\n",
        "        with open(u_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as uf, open(r_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as rf:\n",
        "            u_lines = uf.readlines()\n",
        "            r_lines = rf.readlines()\n",
        "            for u, r in zip(u_lines, r_lines):\n",
        "                u, r = u.strip(), r.strip()\n",
        "                if u and r:\n",
        "                    pairs.append((u, r))\n",
        "    return pd.DataFrame(pairs, columns=[\"Urdu Text\", \"Roman Transliteration\"])\n",
        "\n",
        "# ---------- Text cleaning ----------\n",
        "def clean_urdu(text):\n",
        "    # remove diacritics\n",
        "    text = re.sub(r'[ŸéŸãŸèŸåŸêŸçŸíŸëŸ∞]', '', text)\n",
        "    # normalize alef and variants\n",
        "    text = text.replace(\"ÿ£\", \"ÿß\").replace(\"ÿ•\", \"ÿß\").replace(\"ÿ¢\", \"ÿß\")\n",
        "    # normalize yeh/kaaf etc\n",
        "    text = text.replace(\"Ÿä\", \"€å\").replace(\"ŸÉ\", \"⁄©\").replace(\"⁄æ\", \"€Å\")\n",
        "    # remove non-Arabic/Persian block chars (keep Urdu block)\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "    # collapse spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_roman(text):\n",
        "    text = text.lower()\n",
        "    # keep letters and digits and spaces and basic punctuation maybe, but we will remove punctuation for transliteration\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# ---------- Tokenization & vocab ----------\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CharVocab:\n",
        "    def __init__(self, tokens=None, min_freq=1):\n",
        "        self.min_freq = min_freq\n",
        "        self.idx2token = []\n",
        "        self.token2idx = {}\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        self.build(tokens)\n",
        "\n",
        "    def build(self, tokens):\n",
        "        # tokens: iterable of characters (flattened)\n",
        "        counts = Counter(tokens)\n",
        "        # required specials\n",
        "        specials = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
        "        self.idx2token = specials + [t for t, c in counts.items() if c >= self.min_freq and t not in specials]\n",
        "        self.token2idx = {t: i for i, t in enumerate(self.idx2token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def token2index(self, t):\n",
        "        return self.token2idx.get(t, self.token2idx.get(UNK_TOKEN))\n",
        "\n",
        "    def index2token(self, i):\n",
        "        return self.idx2token[i]\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "def char_tokenize_spaced(text):\n",
        "    # returns list of characters (no spaces removed since we want spaces as token too)\n",
        "    # preserve spaces as actual ' ' tokens\n",
        "    return list(text)\n",
        "\n",
        "class TranslitDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, trg_vocab, max_len=200):\n",
        "        self.src = df[\"Urdu Text\"].tolist()\n",
        "        self.trg = df[\"Roman Transliteration\"].tolist()\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def encode_seq(self, seq, vocab):\n",
        "        # add SOS and EOS for target only outside if needed\n",
        "        ids = [vocab.token2index(ch) for ch in seq]\n",
        "        return ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_chars = char_tokenize_spaced(self.src[idx])\n",
        "        trg_chars = char_tokenize_spaced(self.trg[idx])\n",
        "        src_ids = [self.src_vocab.token2index(ch) for ch in src_chars]\n",
        "        trg_ids = [self.trg_vocab.token2index(ch) for ch in trg_chars]\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch: list of (src_ids, trg_ids)\n",
        "    src_seqs, trg_seqs = zip(*batch)\n",
        "    src_lengths = [len(s) for s in src_seqs]\n",
        "    trg_lengths = [len(t) for t in trg_seqs]\n",
        "    max_src = max(src_lengths)\n",
        "    max_trg = max(trg_lengths) + 2  # for SOS EOS in decoder input/output\n",
        "\n",
        "    padded_src = torch.full((len(batch), max_src), fill_value=src_vocab.token2index(PAD_TOKEN), dtype=torch.long)\n",
        "    padded_trg = torch.full((len(batch), max_trg), fill_value=trg_vocab.token2index(PAD_TOKEN), dtype=torch.long)\n",
        "\n",
        "    for i, (s, t) in enumerate(zip(src_seqs, trg_seqs)):\n",
        "        padded_src[i, :len(s)] = s\n",
        "        # target decoder input: <sos> + trg + <eos>\n",
        "        trg_in = [trg_vocab.token2index(SOS_TOKEN)] + t.tolist() + [trg_vocab.token2index(EOS_TOKEN)]\n",
        "        padded_trg[i, :len(trg_in)] = torch.tensor(trg_in, dtype=torch.long)\n",
        "\n",
        "    src_lengths = torch.tensor(src_lengths, dtype=torch.long)\n",
        "    trg_lengths = torch.tensor([len(t)+2 for t in trg_seqs], dtype=torch.long)\n",
        "    return padded_src.to(DEVICE), src_lengths.to(DEVICE), padded_trg.to(DEVICE), trg_lengths.to(DEVICE)\n",
        "\n",
        "# ---------- Seq2Seq Model ----------\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0.0)\n",
        "        # we'll use a linear to map encoder final states to decoder initial states\n",
        "        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: (B, L)\n",
        "        embedded = self.embedding(src)  # (B, L, E)\n",
        "        # pack padded would be better, but for brevity we won't pack; model handles paddings via embeddings PAD idx\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)  # hidden: (2*n_layers, B, hid_dim)\n",
        "        # Take last layer's forward and backward hidden (for each layer it's organized as (fwd_l1, bwd_l1, fwd_l2, bwd_l2,...))\n",
        "        # We'll extract last forward and backward pair\n",
        "        # hidden shape: (num_directions * n_layers, B, hid_dim)\n",
        "        # For simplicity, take the last two rows (last layer fwd and bwd)\n",
        "        # hidden[-2] = forward last layer, hidden[-1] = backward last layer\n",
        "        h_last_fwd = hidden[-2]  # (B, hid_dim)\n",
        "        h_last_bwd = hidden[-1]  # (B, hid_dim)\n",
        "        c_last_fwd = cell[-2]\n",
        "        c_last_bwd = cell[-1]\n",
        "\n",
        "        h_cat = torch.cat((h_last_fwd, h_last_bwd), dim=1)  # (B, hid_dim*2)\n",
        "        c_cat = torch.cat((c_last_fwd, c_last_bwd), dim=1)\n",
        "\n",
        "        h_dec = torch.tanh(self.fc_hidden(h_cat))  # (B, hid_dim)\n",
        "        c_dec = torch.tanh(self.fc_cell(c_cat))    # (B, hid_dim)\n",
        "\n",
        "        # return full outputs for attention (not used here) and decoder initial states\n",
        "        return outputs, (h_dec.unsqueeze(0), c_dec.unsqueeze(0))  # (1,B,hid) as initial for decoder\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0.0)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.n_layers = n_layers\n",
        "        self.hid_dim = hid_dim\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        # input_token: (B,) single token ids\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # (B,1,E)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (B, output_dim)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, dec_n_layers):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.dec_n_layers = dec_n_layers\n",
        "\n",
        "        # If encoder produces only 1 layer hidden (h_dec shape (1,B,H)), but decoder expects dec_n_layers,\n",
        "        # we replicate/expand encoder state to match decoder layers\n",
        "    def forward(self, src, src_lengths, trg=None, teacher_forcing_ratio=0.5):\n",
        "        # src: (B, L), trg: (B, T) with <sos>... we assume trg includes SOS as first token\n",
        "        batch_size = src.shape[0]\n",
        "        max_trg_len = trg.shape[1] if trg is not None else 200\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_trg_len, trg_vocab_size).to(DEVICE)\n",
        "        enc_outputs, (h_enc, c_enc) = self.encoder(src, src_lengths)  # h_enc: (1,B,H)\n",
        "\n",
        "        # Expand encoder hidden to decoder layers\n",
        "        # h_enc: (1,B,H) -> create (dec_layers,B,H) by repeating\n",
        "        h_init = h_enc.repeat(self.dec_n_layers, 1, 1).contiguous()\n",
        "        c_init = c_enc.repeat(self.dec_n_layers, 1, 1).contiguous()\n",
        "\n",
        "        # first input token is assumed in trg[:,0] as <sos>\n",
        "        input_tok = trg[:, 0]\n",
        "\n",
        "        hidden, cell = h_init, c_init\n",
        "\n",
        "        for t in range(1, max_trg_len):\n",
        "            output, hidden, cell = self.decoder(input_tok, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_tok = trg[:, t] if (trg is not None and teacher_force) else top1\n",
        "        return outputs\n",
        "\n",
        "# ---------- Levenshtein / CER ----------\n",
        "def levenshtein(a, b):\n",
        "    # a, b are lists or strings\n",
        "    if len(a) < len(b):\n",
        "        return levenshtein(b, a)\n",
        "    # now len(a) >= len(b)\n",
        "    previous_row = list(range(len(b) + 1))\n",
        "    for i, ca in enumerate(a, start=1):\n",
        "        current_row = [i]\n",
        "        for j, cb in enumerate(b, start=1):\n",
        "            insertions = previous_row[j] + 1\n",
        "            deletions = current_row[j-1] + 1\n",
        "            substitutions = previous_row[j-1] + (0 if ca == cb else 1)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def cer_score(ref, hyp):\n",
        "    # ref,hyp strings\n",
        "    edits = levenshtein(list(ref), list(hyp))\n",
        "    denom = max(1, len(ref))\n",
        "    return edits / denom\n",
        "\n",
        "# ---------- BLEU function ----------\n",
        "smooth = SmoothingFunction().method4\n",
        "def corpus_bleu_from_lists(references, hypotheses):\n",
        "    # references: list of reference token lists (each inner list: list of reference lists)\n",
        "    # hypotheses: list of hypothesis token lists\n",
        "    scores = []\n",
        "    for refs, hyp in zip(references, hypotheses):\n",
        "        try:\n",
        "            sc = sentence_bleu(refs, hyp, smoothing_function=smooth)\n",
        "        except Exception:\n",
        "            sc = 0.0\n",
        "        scores.append(sc)\n",
        "    return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "# ---------- Experiment runner ----------\n",
        "def build_vocabs(df_all, min_freq=1):\n",
        "    # gather chars from both columns\n",
        "    src_chars = []\n",
        "    trg_chars = []\n",
        "    for s in df_all[\"Urdu Text\"]:\n",
        "        src_chars.extend(list(s))\n",
        "    for t in df_all[\"Roman Transliteration\"]:\n",
        "        trg_chars.extend(list(t))\n",
        "    sv = CharVocab(src_chars, min_freq=min_freq)\n",
        "    tv = CharVocab(trg_chars, min_freq=min_freq)\n",
        "    # ensure specials included\n",
        "    for sp in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n",
        "        if sp not in sv.token2idx:\n",
        "            sv.idx2token.insert(0, sp)\n",
        "            sv.token2idx = {t:i for i,t in enumerate(sv.idx2token)}\n",
        "        if sp not in tv.token2idx:\n",
        "            tv.idx2token.insert(0, sp)\n",
        "            tv.token2idx = {t:i for i,t in enumerate(tv.idx2token)}\n",
        "    return sv, tv\n",
        "\n",
        "def decode_indices(indices, vocab):\n",
        "    # indices: list of ints (without PAD)\n",
        "    tokens = []\n",
        "    for i in indices:\n",
        "        t = vocab.index2token(i)\n",
        "        if t in (PAD_TOKEN, SOS_TOKEN, EOS_TOKEN):\n",
        "            continue\n",
        "        tokens.append(t)\n",
        "    return \"\".join(tokens)\n",
        "\n",
        "def greedy_decode(model, src_batch, src_lengths, max_len=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_size = src_batch.size(0)\n",
        "        # create dummy trg with SOS at position 0 for each\n",
        "        sos_idx = trg_vocab.token2index(SOS_TOKEN)\n",
        "        trg_dummy = torch.full((batch_size, max_len), fill_value=trg_vocab.token2index(PAD_TOKEN), dtype=torch.long).to(DEVICE)\n",
        "        trg_dummy[:,0] = sos_idx\n",
        "        outputs = model(src_batch, src_lengths, trg_dummy, teacher_forcing_ratio=0.0)  # (B, T, V)\n",
        "        preds = outputs.argmax(2).cpu().numpy()  # (B, T)\n",
        "    # convert preds to strings\n",
        "    results = []\n",
        "    for p in preds:\n",
        "        # stop at EOS if appears\n",
        "        chars = []\n",
        "        for idx in p:\n",
        "            tok = trg_vocab.index2token(idx)\n",
        "            if tok == EOS_TOKEN:\n",
        "                break\n",
        "            if tok == PAD_TOKEN or tok == SOS_TOKEN:\n",
        "                continue\n",
        "            chars.append(tok)\n",
        "        results.append(\"\".join(chars))\n",
        "    return results\n",
        "\n",
        "# ---------- Main: prepare data (download extraction is manual here) ----------\n",
        "# If you have dataset.zip in working dir, uncomment download_and_extract() call and adapt for your environment.\n",
        "# unzip_dir = download_and_extract()\n",
        "unzip_dir = \"urdu_dataset\"  # assume dataset already extracted into this folder. If not, extract dataset.zip manually here.\n",
        "if not os.path.exists(unzip_dir):\n",
        "    raise RuntimeError(f\"Please extract dataset.zip into folder '{unzip_dir}' (or run download_and_extract()).\")\n",
        "\n",
        "df = collect_pairs(unzip_dir)\n",
        "print(\"Initial pairs:\", len(df))\n",
        "\n",
        "# Clean\n",
        "df[\"Urdu Text\"] = df[\"Urdu Text\"].apply(clean_urdu)\n",
        "df[\"Roman Transliteration\"] = df[\"Roman Transliteration\"].apply(clean_roman)\n",
        "# drop empty after cleaning\n",
        "df = df[(df[\"Urdu Text\"].str.len() > 0) & (df[\"Roman Transliteration\"].str.len() > 0)].reset_index(drop=True)\n",
        "print(\"After cleaning pairs:\", len(df))\n",
        "\n",
        "# Save full cleaned dataset\n",
        "df.to_csv(\"all_pairs_clean.csv\", index=False)\n",
        "\n",
        "# Build vocabs\n",
        "src_vocab, trg_vocab = build_vocabs(df, min_freq=1)\n",
        "print(\"Src vocab size:\", len(src_vocab), \"Trg vocab size:\", len(trg_vocab))\n",
        "\n",
        "# Add tokens columns for convenience\n",
        "df[\"Urdu Tokens\"] = df[\"Urdu Text\"].apply(lambda x: \" \".join(list(x)))\n",
        "df[\"Roman Tokens\"] = df[\"Roman Transliteration\"].apply(lambda x: \" \".join(list(x)))\n",
        "\n",
        "# ---------- Splits: 50/25/25 ----------\n",
        "train_df, rest_df = train_test_split(df, test_size=0.5, random_state=42)\n",
        "valid_df, test_df = train_test_split(rest_df, test_size=0.5, random_state=42)\n",
        "\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "valid_df.to_csv(\"valid_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)\n",
        "print(\"Saved CSV splits: train/valid/test sizes\", len(train_df), len(valid_df), len(test_df))\n",
        "\n",
        "# ---------- Experiments config ----------\n",
        "experiments = [\n",
        "    {\n",
        "        \"name\": \"exp1\",\n",
        "        \"emb_dim\": 128,\n",
        "        \"hid_dim\": 256,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"exp2\",\n",
        "        \"emb_dim\": 256,\n",
        "        \"hid_dim\": 256,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"exp3\",\n",
        "        \"emb_dim\": 256,\n",
        "        \"hid_dim\": 512,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lr\": 1e-4,\n",
        "        \"batch_size\": 32\n",
        "    }\n",
        "]\n",
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "# ---------- Training / Evaluation ----------\n",
        "for cfg in experiments:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Starting experiment:\", cfg[\"name\"])\n",
        "    print(cfg)\n",
        "    # Build model\n",
        "    encoder = EncoderBiLSTM(input_dim=len(src_vocab), emb_dim=cfg[\"emb_dim\"], hid_dim=cfg[\"hid_dim\"], n_layers=cfg[\"enc_layers\"], dropout=cfg[\"dropout\"], pad_idx=src_vocab.token2index(PAD_TOKEN)).to(DEVICE)\n",
        "    decoder = DecoderLSTM(output_dim=len(trg_vocab), emb_dim=cfg[\"emb_dim\"], hid_dim=cfg[\"hid_dim\"], n_layers=cfg[\"dec_layers\"], dropout=cfg[\"dropout\"], pad_idx=trg_vocab.token2index(PAD_TOKEN)).to(DEVICE)\n",
        "    model = Seq2Seq(encoder, decoder, dec_n_layers=cfg[\"dec_layers\"]).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab.token2index(PAD_TOKEN))\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_dataset = TranslitDataset(train_df, src_vocab, trg_vocab)\n",
        "    val_dataset = TranslitDataset(valid_df, src_vocab, trg_vocab)\n",
        "    test_dataset = TranslitDataset(test_df, src_vocab, trg_vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    best_bleu = 0.0\n",
        "\n",
        "    for epoch in range(1, N_EPOCHS+1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        for src_batch, src_lens, trg_batch, trg_lens in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_batch, src_lens, trg_batch, teacher_forcing_ratio=0.5)  # (B, T, V)\n",
        "            output_dim = output.shape[-1]\n",
        "            # Flatten predictions and targets (ignore first token <sos> in target)\n",
        "            pred = output[:,1:,:].reshape(-1, output_dim)\n",
        "            target = trg_batch[:,1:].reshape(-1)\n",
        "            loss = criterion(pred, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss_sum += loss.item()\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "\n",
        "        # Validate: compute loss + generate outputs for BLEU/CER\n",
        "        model.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "        cer_vals = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src_batch, src_lens, trg_batch, trg_lens in val_loader:\n",
        "                output = model(src_batch, src_lens, trg_batch, teacher_forcing_ratio=0.0)  # no TF\n",
        "                output_dim = output.shape[-1]\n",
        "                pred = output[:,1:,:].reshape(-1, output_dim)\n",
        "                target = trg_batch[:,1:].reshape(-1)\n",
        "                loss = criterion(pred, target)\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # greedy decode for metrics\n",
        "                hyps = greedy_decode(model, src_batch, src_lens, max_len=trg_batch.size(1))\n",
        "                # build references\n",
        "                for i in range(trg_batch.size(0)):\n",
        "                    # reconstruct reference string (remove SOS/EOS/PAD)\n",
        "                    trg_ids = trg_batch[i].cpu().numpy().tolist()\n",
        "                    # remove SOS\n",
        "                    if len(trg_ids) > 0 and trg_ids[0] == trg_vocab.token2index(SOS_TOKEN):\n",
        "                        trg_ids = trg_ids[1:]\n",
        "                    # collect until EOS or PAD\n",
        "                    ref_chars = []\n",
        "                    for idx in trg_ids:\n",
        "                        tok = trg_vocab.index2token(idx)\n",
        "                        if tok == EOS_TOKEN or tok == PAD_TOKEN:\n",
        "                            break\n",
        "                        if tok == SOS_TOKEN:\n",
        "                            continue\n",
        "                        ref_chars.append(tok)\n",
        "                    references.append([ref_chars])  # sentence_bleu expects list of references\n",
        "                hypotheses.extend([list(h) for h in hyps])\n",
        "                # CER per sample\n",
        "                for ref_chars, hyp_str in zip(references[-trg_batch.size(0):], hyps):\n",
        "                    # ref_chars is [[c1,c2,...]] so pick first\n",
        "                    ref_str = \"\".join(ref_chars[0])\n",
        "                    cer_vals.append(cer_score(ref_str, hyp_str))\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        perplexity = math.exp(val_loss) if val_loss < 700 else float(\"inf\")  # avoid overflow\n",
        "\n",
        "        # Compute BLEU over the whole validation set (references/hypotheses assembled)\n",
        "        # Note: our `references` list has one element per validation sequence (list of lists)\n",
        "        # but we appended in loop accumulating; make sure lengths match\n",
        "        # If lengths mismatch, take min\n",
        "        n_items = min(len(references), len(hypotheses))\n",
        "        refs_trim = references[:n_items]\n",
        "        hyps_trim = hypotheses[:n_items]\n",
        "        # convert hyps_trim to lists of tokens\n",
        "        hyp_tokens = [list(h) for h in hyps_trim]\n",
        "        bleu = corpus_bleu_from_lists(refs_trim, hyp_tokens)\n",
        "        avg_cer = sum(cer_vals)/len(cer_vals) if cer_vals else 0.0\n",
        "\n",
        "        print(f\"[{cfg['name']}] Epoch {epoch}/{N_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | PPL: {perplexity:.3f} | BLEU: {bleu:.4f} | CER: {avg_cer:.4f}\")\n",
        "\n",
        "        # print a few qualitative examples\n",
        "        print(\"Some validation examples (src => pred | ref):\")\n",
        "        # pick first batch from val_loader for demonstration\n",
        "        for i in range(3):\n",
        "            # random sample from val dataset\n",
        "            idx = random.randint(0, len(val_dataset)-1)\n",
        "            src_str = val_dataset.src[idx]\n",
        "            ref_str = val_dataset.trg[idx]\n",
        "            # encode single example\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                s_ids = torch.tensor([ [src_vocab.token2index(ch) for ch in list(src_str)] ], dtype=torch.long).to(DEVICE)\n",
        "                s_len = torch.tensor([s_ids.size(1)], dtype=torch.long).to(DEVICE)\n",
        "                pred_list = greedy_decode(model, s_ids, s_len, max_len=200)\n",
        "            print(f\"{src_str} => {pred_list[0]} | {ref_str}\")\n",
        "\n",
        "    # After training each experiment, you can optionally save the model\n",
        "    torch.save(model.state_dict(), f\"seq2seq_{cfg['name']}.pth\")\n",
        "    print(f\"Saved model seq2seq_{cfg['name']}.pth\")\n",
        "\n",
        "print(\"All experiments complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu7xwh2ZxA-E",
        "outputId": "85e51254-b74f-421a-b2d3-48b753ed888b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Initial pairs: 21003\n",
            "After cleaning pairs: 21003\n",
            "Src vocab size: 53 Trg vocab size: 40\n",
            "Saved CSV splits: train/valid/test sizes 10501 5251 5251\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp1\n",
            "{'name': 'exp1', 'emb_dim': 128, 'hid_dim': 256, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 64}\n",
            "[exp1] Epoch 1/5 | Train Loss: 3.0083 | Val Loss: 2.9997 | PPL: 20.081 | BLEU: 0.0222 | CER: 1.0667\n",
            "Some validation examples (src => pred | ref):\n",
            "ŸÖ€å⁄∫ ŸÜ€í ŸÖÿØÿ™ ÿ≥€í ⁄©Ÿàÿ¶€å ÿÆŸàÿßÿ® ŸÜ€Å€å⁄∫ ÿØ€å⁄©€Åÿß €Å€í => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | mai√± ne muddat se koƒ´ ·∏≥hvƒÅb nahƒ´√± dekhƒÅ hai\n",
            "ŸæŸÑ⁄©Ÿà⁄∫ Ÿæ€Å ⁄©⁄Ü€å ŸÜ€åŸÜÿØŸà⁄∫ ⁄©ÿß ÿ±ÿ≥ Ÿæ€Å€åŸÑÿ™ÿß €ÅŸà ÿ¨ÿ® => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | palko√± pe kachchƒ´ nƒ´√±do√± kƒÅ ras phailtƒÅ ho jab\n",
            "ÿ¨Ÿà ÿ®€í ÿ´ÿ®ÿßÿ™ €ÅŸà ÿßÿ≥ ÿ≥ÿ± ÿÆŸàÿ¥€å ⁄©Ÿà ⁄©€åÿß ⁄©€åÿ¨€í => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | jo besabƒÅt ho us sar·∏≥hushƒ´ ko kyƒÅ kiije\n",
            "[exp1] Epoch 2/5 | Train Loss: 2.7256 | Val Loss: 2.9792 | PPL: 19.672 | BLEU: 0.0337 | CER: 0.8315\n",
            "Some validation examples (src => pred | ref):\n",
            "ÿ¨Ÿà €å€Åÿß⁄∫ ÿ≥€í ⁄©€Å€å⁄∫ ŸÜ€Å ÿ¨ÿßÿ™ÿß ÿ™€Åÿß => ka   aa   aa   aa   aa   aa  aa  aa  aa  | jo yahƒÅ√± se kahƒ´√± na jaatƒÅ thƒÅ\n",
            "ÿØŸÑ ÿ¨Ÿà €Å€í ÿß⁄Ø ŸÑ⁄Øÿß ÿØŸà⁄∫ ÿßÿ≥ ⁄©Ÿà => dar   aa   aa   aa   aa   aa  aa  aa  aa  | dil jo hai aag lagƒÅ duu√± us ko\n",
            "ŸÜ⁄Øÿß€Å ÿπ⁄©ÿ≥ ŸÅÿ±Ÿàÿ¥ Ÿà ÿÆ€åÿßŸÑ ÿßÿ¶ŸÜ€Å ÿ≥ÿßÿ≤ => na ha   aa   aa   aa   aa  aa  aa  aa  aa | nigƒÅh aksfarosh o ·∏≥hayƒÅl ƒÅinasƒÅz\n",
            "[exp1] Epoch 3/5 | Train Loss: 2.6185 | Val Loss: 2.9291 | PPL: 18.710 | BLEU: 0.0425 | CER: 0.7656\n",
            "Some validation examples (src => pred | ref):\n",
            "ÿ¨Ÿà ÿØ€í ÿ≥⁄©ÿß ŸÜ€Å Ÿæ€Åÿß⁄ëŸà⁄∫ ⁄©Ÿà ÿ®ÿ±ŸÅ ⁄©€å ⁄ÜÿßÿØÿ± => ja  a  a  aa  a  aa  a  aa  aa  aa  aa  aa | jo de sakƒÅ na pahƒÅ·∏ço√± ko barf kƒ´ chƒÅdar\n",
            "ŸÖÿ™ ÿ≥€ÅŸÑ €ÅŸÖ€å⁄∫ ÿ¨ÿßŸÜŸà Ÿæ€Åÿ±ÿ™ÿß €Å€í ŸÅŸÑ⁄© ÿ®ÿ±ÿ≥Ÿà⁄∫ => mar  aa  a  aa  a  aa  aa  aa  aa  aa  aa  | mat sahl hame√± jaano phirtƒÅ hai falak barso√±\n",
            "ÿ≥€åÿß€Å€å €Å€í ŸÖÿ±€í ÿß€åÿßŸÖ ŸÖ€å⁄∫ ŸÑŸàÿ≠ ÿØÿ®ÿ≥ÿ™ÿß⁄∫ ⁄©€å => saar  aa  a  aa  a  aa  aa  aa  aa  aa | siyƒÅhƒ´ hai mire ayyƒÅm me√± lauhedabistƒÅ√± kƒ´\n",
            "[exp1] Epoch 4/5 | Train Loss: 2.5449 | Val Loss: 2.9011 | PPL: 18.195 | BLEU: 0.0509 | CER: 0.7565\n",
            "Some validation examples (src => pred | ref):\n",
            "⁄©€Å ÿÆŸàÿßÿ® ÿ®€Å€å ŸÖÿ±€í ÿ±ÿÆÿµÿ™ €Å€å⁄∫ ÿ±ÿ™ÿ¨⁄Øÿß ÿ®€Å€å ⁄Ø€åÿß => kahƒ´√± ka  aa   aa  aa   aa  aa   aa  aa  aa  aa  | ki ·∏≥hvƒÅb bhƒ´ mire ru·∏≥hsat hai√± ratjagƒÅ bhƒ´ gayƒÅ\n",
            "ÿ¥ÿ±⁄© ⁄Ü€ÅŸà⁄ëÿß ÿ™Ÿà ÿ≥ÿ® ŸÜ€í ⁄Ü€ÅŸà⁄ë ÿØ€åÿß => sharƒÅ  aa  aa   aa  aa  aa  aa  aa  aa  aa  aa | shirk chho·∏çƒÅ to sab ne chho·∏ç diyƒÅ\n",
            "⁄©ÿßÿ∫ÿ∞ ⁄©ÿß €å€Å ÿ¥€Åÿ± ÿß⁄ë ŸÜ€Å ÿ¨ÿßÿ¶€í => kyƒÅ ka  aa   aa  aa   aa  aa   aa  aa  aa  aa  aa | kƒÅƒ°haz kƒÅ ye shahr u·∏ç na jaae\n",
            "[exp1] Epoch 5/5 | Train Loss: 2.4968 | Val Loss: 2.8758 | PPL: 17.739 | BLEU: 0.0657 | CER: 0.6958\n",
            "Some validation examples (src => pred | ref):\n",
            "€å€Å ŸÖÿ∑ŸÑÿπ ÿßÿ≥ÿØÿî ÿ¨Ÿà€Åÿ± ÿßŸÅÿ≥ŸàŸÜ ÿ≥ÿÆŸÜ €ÅŸà => ye ka  a  a  a  a  a  a  a  a  a  a  a hai | ye matla asad jauhareafs≈´nesu·∏≥han ho\n",
            "ÿ¥ŸàÿÆ€å ÿ≥€å €Å€í ÿ≥ŸàÿßŸÑ ŸÖ⁄©ÿ±ÿ± ŸÖ€å⁄∫ ÿß€í ⁄©ŸÑ€åŸÖ => shaaa  a  a  a  a  a  a  a  a  a  a  aa  aa  | sho·∏≥hƒ´ sƒ´ hai savƒÅlemukarrar me√± ai kalƒ´m\n",
            "ÿ¨ÿß ⁄©€í Ÿæ€ÅŸÜ⁄Ü€å €Å€í ÿ≠ÿØ ÿ∏ŸÑŸÖÿ™ ⁄©Ÿà⁄∫ => jaat  aa  a  a  a  a  a  a  a  a  a  a  aa  aa  | jƒÅ ke pahu√±chƒ´ hai haddezulmat kuu√±\n",
            "Saved model seq2seq_exp1.pth\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp2\n",
            "{'name': 'exp2', 'emb_dim': 256, 'hid_dim': 256, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.1, 'lr': 0.0005, 'batch_size': 64}\n",
            "[exp2] Epoch 1/5 | Train Loss: 3.0402 | Val Loss: 2.9852 | PPL: 19.790 | BLEU: 0.0154 | CER: 0.9726\n",
            "Some validation examples (src => pred | ref):\n",
            "⁄ØŸÑ ŸÅÿ¥ÿßŸÜ€å €Åÿßÿ¶€í ŸÜÿßÿ≤ ÿ¨ŸÑŸà€Å ⁄©Ÿà ⁄©€åÿß €ÅŸà ⁄Ø€åÿß => ka                                         | gulfishƒÅnƒ´hƒÅenƒÅzejalva ko kyƒÅ ho gayƒÅ\n",
            "ÿßÿ≥ÿØÿî €å€Å ÿπÿ¨ÿ≤ Ÿà ÿ®€í ÿ≥ÿßŸÖÿßŸÜ€åŸî ŸÅÿ±ÿπŸàŸÜ ÿ™ŸàÿßŸÖ €Å€í => ka                                         | asad ye ijzobesƒÅmƒÅnƒ´efiraunetauam hai\n",
            "ÿØ€å⁄©€Å€å€í €ÅŸàÿ™€å €Å€í ⁄©ÿ® ÿ±ÿß€Å€å ÿ≥Ÿàÿ¶€í ŸÖŸÑ⁄© ÿπÿØŸÖ => ka                                         | dekhiye hotƒ´ hai kab raahƒ´ s≈´emulkeadam\n",
            "[exp2] Epoch 2/5 | Train Loss: 2.8894 | Val Loss: 2.9748 | PPL: 19.586 | BLEU: 0.0469 | CER: 0.7718\n",
            "Some validation examples (src => pred | ref):\n",
            "ŸÖÿ¨ÿ®Ÿàÿ± €åÿß⁄∫ ÿ™ŸÑ⁄© €ÅŸàÿ¶€í ÿß€í ÿßÿÆÿ™€åÿßÿ± ÿ≠€åŸÅ => mar  a   a  a  a  a  a  a  a  a ha  haa | majb≈´r yaa√± talak hue ai i·∏≥htiyƒÅr haif\n",
            "€Åÿ¨ÿ± €ÅŸà €åÿß ŸàÿµÿßŸÑ €ÅŸà ⁄©⁄Ü€Å €ÅŸà => ha             a  a  a  a  a  aa ha haa | hijr ho yƒÅ visƒÅl ho kuchh ho\n",
            "ÿ¥ÿ® ÿ™€åÿ±€Å ŸÖ€å⁄∫ ÿ¥ŸÖÿπ ÿ±Ÿàÿ¥ŸÜ ⁄©Ÿà => saha  a  a  a  a  a  a  a  a  a ha  haa haa | shabetƒ´ra me√± shama raushan ko\n",
            "[exp2] Epoch 3/5 | Train Loss: 2.6824 | Val Loss: 2.9555 | PPL: 19.211 | BLEU: 0.0556 | CER: 0.8310\n",
            "Some validation examples (src => pred | ref):\n",
            "€å€Å€å ÿØÿßÿ∫ ÿ™€Å€í ÿ¨Ÿà ÿ≥ÿ¨ÿß ⁄©€í €ÅŸÖ ÿ≥ÿ± ÿ®ÿ≤ŸÖ €åÿßÿ± ⁄ÜŸÑ€í ⁄Øÿ¶€í => ya ha    ha    ha    ha   ha   ha   ha ha ha ha haa | yahƒ´ daaƒ°h the jo sajƒÅ ke ham sarebazmeyƒÅr chale gae\n",
            "ŸàŸÇÿ™ ⁄©€å ÿ∂ÿ±ÿ® ÿ≥€í ⁄©Ÿπ ÿ¨ÿßÿ™€í €Å€å⁄∫ ÿ≥ÿ® ⁄©€í ÿ≥€åŸÜ€í => va ha    ha    ha    ha   ha   ha  ha ha ha ha haa | vaqt kƒ´ zarb se kat jaate hai√± sab ke siine\n",
            "ÿß⁄Øÿ±⁄Ü€Å ÿ≥€åŸÜ€í ŸÖ€å⁄∫ ÿ≥ÿßŸÜÿ≥ ÿ®€Å€å €Å€í ŸÜ€Å€å⁄∫ ÿ∑ÿ®€åÿπÿ™ ŸÖ€å⁄∫ ÿ¨ÿßŸÜ ÿ®ÿßŸÇ€å => as    ha   aa   aa   aa   ha   ha  ha ha ha haa | agarche siine me√± saa√±s bhƒ´ hai nahƒ´√± tabƒ´at me√± jaan baaqƒ´\n",
            "[exp2] Epoch 4/5 | Train Loss: 2.5987 | Val Loss: 2.8772 | PPL: 17.765 | BLEU: 0.0565 | CER: 0.7842\n",
            "Some validation examples (src => pred | ref):\n",
            "ÿßÿ≥ ÿ¥€Åÿ± ÿÆÿ±ÿßÿ®€å ŸÖ€å⁄∫ ÿ∫ŸÖ ÿπÿ¥ŸÇ ⁄©€í ŸÖÿßÿ±€í => as ka                          ha    ha   aa  aa | is shahre·∏≥harƒÅbƒ´ me√± ƒ°hameishq ke maare\n",
            "⁄©ÿ®€Å€å ŸæŸÑ⁄©Ÿà⁄∫ Ÿæ€Å ⁄ÜŸÖ⁄©ÿ™€å €Å€í ÿ¨Ÿà ÿßÿ¥⁄©Ÿà⁄∫ ⁄©€å ŸÑ⁄©€åÿ± => kash    aa                         ha    ha   aa  aa | kabhƒ´ palko√± pe chamaktƒ´ hai jo ashko√± kƒ´ lakƒ´r\n",
            "ŸÖÿ¨€Å€í ÿ™Ÿà ÿ±Ÿàÿ≤ ⁄©ÿ≥ŸàŸπ€å Ÿæ€Å ÿØÿ±ÿØ ⁄©ÿ≥ÿ™ÿß €Å€í => mash    aa                        ha    ha   aa  aa | mujhe to roz kasautƒ´ pe dard kastƒÅ hai\n",
            "[exp2] Epoch 5/5 | Train Loss: 2.5126 | Val Loss: 2.9045 | PPL: 18.256 | BLEU: 0.0839 | CER: 0.6746\n",
            "Some validation examples (src => pred | ref):\n",
            "ŸÖ⁄Øÿ± €å€Å ÿ≥ÿ® €Å€í ŸÖÿ¨€Å€í ŸÜÿß⁄ØŸàÿßÿ± ÿπ€åÿØ ⁄©€í ÿØŸÜ => marar ka  aa  aa  aa  aa  aa  aa  aa  aahƒÅ√± | magar ye sab hai mujhe nƒÅgavƒÅr eid ke din\n",
            "ÿ±⁄©€Å ŸÑ€åÿ¨Ÿà ŸÖ€åÿ±€í ÿØÿπŸà€å Ÿàÿßÿ±ÿ≥ÿ™⁄Ø€å ⁄©€å ÿ¥ÿ±ŸÖ => rakh  aa  aa  aa  aa  aa  aa  aa  aa  aahƒÅ | rakh liijo mere dƒÅvaevƒÅrastagƒ´ kƒ´ sharm\n",
            "ÿ±€Åÿß ÿ®ŸÑÿß ŸÖ€å⁄∫ ÿ®€Å€å ŸÖ€å⁄∫ ŸÖÿ®ÿ™ŸÑÿßÿ¶€í ÿßŸÅÿ™ ÿ±ÿ¥⁄© => rahh  aa  aa  aa  aa  aa  aa  aa  aa  aahƒÅ | rahƒÅ balƒÅ me√± bhƒ´ mai√± mubtalaeƒÅfaterashk\n",
            "Saved model seq2seq_exp2.pth\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp3\n",
            "{'name': 'exp3', 'emb_dim': 256, 'hid_dim': 512, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.0001, 'batch_size': 32}\n",
            "[exp3] Epoch 1/5 | Train Loss: 3.0403 | Val Loss: 2.9814 | PPL: 19.716 | BLEU: 0.0157 | CER: 0.9357\n",
            "Some validation examples (src => pred | ref):\n",
            "€Å€å⁄∫ ÿß€ÅŸÑ ÿÆÿ±ÿØ ⁄©ÿ≥ ÿ±Ÿàÿ¥ ÿÆÿßÿµ Ÿæ€Å ŸÜÿßÿ≤ÿß⁄∫ => ka                                       | hai√± ahle·∏≥hirad kis ravishe·∏≥hƒÅs pe nƒÅzƒÅ√±\n",
            "⁄Üÿ±ÿßÿ∫ €ÅŸÖ ŸÜ€í ÿ¨ŸÑÿßÿ¶€í €ÅŸàÿß ⁄©€í ÿ±ÿ≥ÿ™€í ŸÖ€å⁄∫ => ka                                       | charƒÅƒ°h ham ne jalƒÅe havƒÅ ke raste me√±\n",
            "ÿÆÿßÿ± ÿÆÿßÿ± ÿßŸÑŸÖ ÿ≠ÿ≥ÿ±ÿ™ ÿØ€åÿØÿßÿ± ÿ™Ÿà €Å€í => ka                                       | ·∏≥haar ·∏≥hƒÅrealamehasratedƒ´dƒÅr to hai\n",
            "[exp3] Epoch 2/5 | Train Loss: 2.9182 | Val Loss: 2.9777 | PPL: 19.642 | BLEU: 0.0460 | CER: 0.7735\n",
            "Some validation examples (src => pred | ref):\n",
            "⁄©€ÅŸà ⁄Ø€åÿß ÿßÿ¨ ⁄©€Åÿß⁄∫ ÿ±ÿ≤ŸÇ ⁄©ÿß ÿØ€åŸÜ€í ŸàÿßŸÑÿß => ka    aa   aa   aa   aa   aa  aa  aa  aa ha | kho gayƒÅ aaj kahƒÅ√± rizq kƒÅ dene vaalƒÅ\n",
            "ÿßŸæŸÜ€í ŸÖÿßÿ∂€å ⁄©€å ÿ¨ÿ≥ÿ™ÿ¨Ÿà ŸÖ€å⁄∫ ÿ®€Åÿßÿ± => aa   aa   aa   aa   aa   aa  aa  aa  aa ha | apne maazƒ´ kƒ´ justuj≈´ me√± bahƒÅr\n",
            "ÿ¨ŸÅÿß €ÅŸà €åÿß ŸàŸÅÿß €ÅŸÖ ÿ≥ÿ® ŸÖ€å⁄∫ ÿÆŸàÿ¥ €Å€å⁄∫ => mar  a  a  a  a  a  a  aa  aa  aa  aa  aa ha | jafƒÅ ho yƒÅ vafƒÅ ham sab me√± ·∏≥hush hai√±\n",
            "[exp3] Epoch 3/5 | Train Loss: 2.7382 | Val Loss: 3.0070 | PPL: 20.226 | BLEU: 0.0602 | CER: 0.7029\n",
            "Some validation examples (src => pred | ref):\n",
            "ŸàŸÇÿ™ ⁄©€å ÿ∂ÿ±ÿ® ÿ≥€í ⁄©Ÿπ ÿ¨ÿßÿ™€í €Å€å⁄∫ ÿ≥ÿ® ⁄©€í ÿ≥€åŸÜ€í => mar  a  aa   aa   aa   aa   aa   aa   hai | vaqt kƒ´ zarb se kat jaate hai√± sab ke siine\n",
            "ŸÖ⁄Øÿ± €å€Å ÿ¥Ÿà€Åÿ±Ÿà⁄∫ ÿ≥€í ÿßŸæŸÜ€í ÿ®€í Ÿæÿ±Ÿàÿß ŸÜ€Å€å⁄∫ €ÅŸàÿ™€å⁄∫ => mar  a  aa   aa   aa   aa   aa   aa   hai | magar ye shauharo√± se apne beparvƒÅ nahƒ´√± hotƒ´√±\n",
            "ÿπŸÖÿ± ÿ®€Åÿ± €ÅŸÖ ÿ±€Å€í ÿ¥ÿ±ÿßÿ®€å ÿ≥€í => as  aa   aa   aa   aa   aa   aa   aa  hai | umr bhar ham rahe sharƒÅbƒ´ se\n",
            "[exp3] Epoch 4/5 | Train Loss: 2.6540 | Val Loss: 2.9330 | PPL: 18.784 | BLEU: 0.0436 | CER: 0.7345\n",
            "Some validation examples (src => pred | ref):\n",
            "ÿ™ÿ¨€Å ÿ≥€í ÿØŸà ⁄Üÿßÿ± €ÅŸàŸÜ€í ⁄©€å ÿ≠ÿ≥ÿ±ÿ™ ⁄©€í ŸÖÿ®ÿ™ŸÑÿß => mar   aa    aa    aa    aa    aa   aa   aa   hai | tujh se dochƒÅr hone kƒ´ hasrat ke mubtilƒÅ\n",
            "ÿ¨ÿ® ÿ≥ÿ± ŸÖ€å⁄∫ €ÅŸàÿßÿ¶€í ÿ∑ÿßÿπÿ™ ÿ™€Å€å ÿ≥ÿ±ÿ≥ÿ®ÿ≤ ÿ¥ÿ¨ÿ± ÿßŸÖ€åÿØ ⁄©ÿß ÿ™€Åÿß => ji   aa    aa    aa    aa    aa   aa   aa   aa  aa | jab sar me√± havƒÅetƒÅat thƒ´ sarsabz shajar ummƒ´d kƒÅ thƒÅ\n",
            "ÿß€åÿ≥€í ÿ¨€ÅŸàŸπ€í ⁄©Ÿà ÿßŸàÿ± ⁄©€åÿß ⁄©€Åÿ¶€í => as ka  a    ha   aa    aa   aa   aa   aa  aa | aise jh≈´te ko aur kyƒÅ kahiye\n",
            "[exp3] Epoch 5/5 | Train Loss: 2.5826 | Val Loss: 2.9298 | PPL: 18.725 | BLEU: 0.0503 | CER: 0.8610\n",
            "Some validation examples (src => pred | ref):\n",
            "⁄©ÿπÿ®€Å ⁄©ÿ≥ ŸÖŸÜ€Å ÿ≥€í ÿ¨ÿßŸà⁄Ø€í ÿ∫ÿßŸÑÿ®ÿî => kai  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aaar | kaaba kis mu√±h se jƒÅoge ƒ°hƒÅlib\n",
            "⁄©ÿ≥€å ⁄Üÿ±ÿßÿ∫ ⁄©€í ÿ®ÿ≥ ŸÖ€å⁄∫ ÿØ€ÅŸàÿß⁄∫ ŸÜ€Å€å⁄∫ €ÅŸàÿ™ÿß => kis  a  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aaar hai | kisƒ´ charƒÅƒ°h ke bas me√± dhuƒÅ√± nahƒ´√± hotƒÅ\n",
            "ÿßÿ≥ ÿ≥€í ÿØŸÑ ÿ™ÿ®ÿß€Å ⁄©€å ÿ±ŸàÿØÿßÿØ ⁄©€åÿß ⁄©€ÅŸà⁄∫ => as ka  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa hai | us se diletabƒÅh kƒ´ r≈´dƒÅd kyƒÅ kah≈´√±\n",
            "Saved model seq2seq_exp3.pth\n",
            "All experiments complete.\n"
          ]
        }
      ]
    }
  ]
}
