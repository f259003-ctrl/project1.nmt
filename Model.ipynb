{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0c1wk4dvSWe",
        "outputId": "b764ffd6-4c0a-4be1-d92f-2596f3fb4e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 04:50:35--  https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/amir9ume/urdu_ghazals_rekhta/main/dataset/dataset.zip [following]\n",
            "--2025-09-22 04:50:36--  https://raw.githubusercontent.com/amir9ume/urdu_ghazals_rekhta/main/dataset/dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2927519 (2.8M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   2.79M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-09-22 04:50:36 (33.6 MB/s) - ‘dataset.zip’ saved [2927519/2927519]\n",
            "\n",
            "✅ Dataset extracted at: urdu_dataset\n",
            "📊 Urdu files: 1314\n",
            "📊 Roman files: 1314\n",
            "✅ Total pairs: 21003\n",
            "✅ Cleaned sample:\n",
            "                                  Urdu Text  \\\n",
            "6790          تمام حسن کے جلوے تمام محرومی   \n",
            "10823  اب دل کو خدا رکھے اب دل کا زمانا ہے   \n",
            "4680     ابھی سن ہی کیا ہے جو بیباکیاں ہوں   \n",
            "\n",
            "                             Roman Transliteration  \n",
            "6790             tamām husn ke jalve tamām mahrūmī  \n",
            "10823  ab dil ko ḳhudā rakkhe ab dil kā zamānā hai  \n",
            "4680          abhī sin hī kyā hai jo bebākiyāñ hoñ  \n",
            "📂 Tokenized sample:\n",
            "                                               Urdu Text  \\\n",
            "2239  تو پیر مے خانہ سن کے کہنے لگا کہ منہ پھٹ ہے خا...   \n",
            "5097                       دم عتاب جو رنگت تری نکلتی ہے   \n",
            "6384                   کہتا ہوں اسے میں تو خصوصیت پنہاں   \n",
            "\n",
            "                                  Roman Transliteration  \\\n",
            "2239  to piir maiḳhāna sun ke kahne lagā ki muñh pha...   \n",
            "5097                dameitāb jo rañgat tirī nikaltī hai   \n",
            "6384          kahtā huuñ use maiñ to ḳhusūsiyyatepinhāñ   \n",
            "\n",
            "                                            Urdu Tokens  \\\n",
            "2239  ت و   پ ی ر   م ے   خ ا ن ہ   س ن   ک ے   ک ہ ...   \n",
            "5097  د م   ع ت ا ب   ج و   ر ن گ ت   ت ر ی   ن ک ل ...   \n",
            "6384  ک ہ ت ا   ہ و ں   ا س ے   م ی ں   ت و   خ ص و ...   \n",
            "\n",
            "                                           Roman Tokens  \n",
            "2239  t o   p i i r   m a i ḳ h ā n a   s u n   k e ...  \n",
            "5097  d a m e i t ā b   j o   r a ñ g a t   t i r ī ...  \n",
            "6384  k a h t ā   h u u ñ   u s e   m a i ñ   t o   ...  \n",
            "✅ Split sizes:\n",
            "Train: 10501\n",
            "Validation: 5251\n",
            "Test: 5251\n",
            "📂 CSV files saved: train_data.csv, valid_data.csv, test_data.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import string\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Download & Extract Dataset\n",
        "# -------------------------------\n",
        "dataset_url = \"https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\"\n",
        "!wget -O dataset.zip \"$dataset_url\"\n",
        "\n",
        "unzip_dir = \"urdu_dataset\"\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n",
        "\n",
        "print(\"✅ Dataset extracted at:\", unzip_dir)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Collect Files (Urdu + Roman)\n",
        "# -------------------------------\n",
        "urdu_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"ur\", \"*\"), recursive=True))\n",
        "roman_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"en\", \"*\"), recursive=True))\n",
        "\n",
        "print(\"📊 Urdu files:\", len(urdu_files))\n",
        "print(\"📊 Roman files:\", len(roman_files))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Create Urdu-Roman Pairs\n",
        "# -------------------------------\n",
        "pairs = []\n",
        "for ur_path, ro_path in zip(urdu_files, roman_files):\n",
        "    with open(ur_path, \"r\", encoding=\"utf-8\") as ur, open(ro_path, \"r\", encoding=\"utf-8\") as ro:\n",
        "        ur_lines, ro_lines = ur.readlines(), ro.readlines()\n",
        "        for u, r in zip(ur_lines, ro_lines):\n",
        "            u, r = u.strip(), r.strip()\n",
        "            if u and r:\n",
        "                pairs.append((u, r))\n",
        "\n",
        "df = pd.DataFrame(pairs, columns=[\"Urdu Text\", \"Roman Transliteration\"])\n",
        "print(\"✅ Total pairs:\", len(df))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Clean Urdu & Roman Text\n",
        "# -------------------------------\n",
        "def clean_urdu(text):\n",
        "    text = re.sub(r'[ًٌٍَُِّْٰ]', '', text)  # remove diacritics\n",
        "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")  # normalize alef\n",
        "    text = text.replace(\"ي\", \"ی\").replace(\"ك\", \"ک\")  # normalize Yeh, Kaaf\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # keep only Urdu letters\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def clean_roman(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df[\"Urdu Text\"] = df[\"Urdu Text\"].apply(clean_urdu)\n",
        "df[\"Roman Transliteration\"] = df[\"Roman Transliteration\"].apply(clean_roman)\n",
        "\n",
        "print(\"✅ Cleaned sample:\\n\", df.sample(3))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Tokenization (character-level for transliteration)\n",
        "# -------------------------------\n",
        "def char_tokenize(text):\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "df[\"Urdu Tokens\"] = df[\"Urdu Text\"].apply(char_tokenize)\n",
        "df[\"Roman Tokens\"] = df[\"Roman Transliteration\"].apply(char_tokenize)\n",
        "\n",
        "print(\"📂 Tokenized sample:\\n\", df.sample(3))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Train/Validation/Test Split (50/25/25)\n",
        "# -------------------------------\n",
        "train_data, temp_data = train_test_split(df, test_size=0.50, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.50, random_state=42)\n",
        "\n",
        "print(\"✅ Split sizes:\")\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(valid_data))\n",
        "print(\"Test:\", len(test_data))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 7: Save Splits to CSV\n",
        "# -------------------------------\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "valid_data.to_csv(\"valid_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "print(\"📂 CSV files saved: train_data.csv, valid_data.csv, test_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline: dataset -> preprocess -> char vocab -> Seq2Seq (BiLSTM enc, LSTM dec) -> train (5 epochs) -> eval metrics\n",
        "# Save as run_transliteration_experiments.py and run, or paste into a notebook cell.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import glob\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "def download_and_extract():\n",
        "    dataset_url = \"https://github.com/amir9ume/urdu_ghazals_rekhta/raw/main/dataset/dataset.zip\"\n",
        "    if not os.path.exists(\"dataset.zip\"):\n",
        "        print(\"Downloading dataset.zip ...\")\n",
        "        # If running in a shell-enabled environment like Colab, uncomment:\n",
        "        # !wget -O dataset.zip \"$dataset_url\"\n",
        "        # If wget unavailable, the user should provide dataset.zip manually.\n",
        "        raise RuntimeError(\"Please download dataset.zip manually into working directory or enable shell commands.\")\n",
        "    print(\"Extracting dataset.zip ...\")\n",
        "    unzip_dir = \"urdu_dataset\"\n",
        "    os.makedirs(unzip_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)\n",
        "    return unzip_dir\n",
        "\n",
        "def collect_pairs(unzip_dir):\n",
        "    urdu_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"ur\", \"*\"), recursive=True))\n",
        "    roman_files = sorted(glob.glob(os.path.join(unzip_dir, \"dataset\", \"**\", \"en\", \"*\"), recursive=True))\n",
        "    pairs = []\n",
        "    for u_path, r_path in zip(urdu_files, roman_files):\n",
        "        with open(u_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as uf, open(r_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as rf:\n",
        "            u_lines = uf.readlines()\n",
        "            r_lines = rf.readlines()\n",
        "            for u, r in zip(u_lines, r_lines):\n",
        "                u, r = u.strip(), r.strip()\n",
        "                if u and r:\n",
        "                    pairs.append((u, r))\n",
        "    return pd.DataFrame(pairs, columns=[\"Urdu Text\", \"Roman Transliteration\"])\n",
        "\n",
        "# ---------- Text cleaning ----------\n",
        "def clean_urdu(text):\n",
        "    # remove diacritics\n",
        "    text = re.sub(r'[ًٌٍَُِّْٰ]', '', text)\n",
        "    # normalize alef and variants\n",
        "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
        "    # normalize yeh/kaaf etc\n",
        "    text = text.replace(\"ي\", \"ی\").replace(\"ك\", \"ک\").replace(\"ھ\", \"ہ\")\n",
        "    # remove non-Arabic/Persian block chars (keep Urdu block)\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "    # collapse spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_roman(text):\n",
        "    text = text.lower()\n",
        "    # keep letters and digits and spaces and basic punctuation maybe, but we will remove punctuation for transliteration\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# ---------- Tokenization & vocab ----------\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CharVocab:\n",
        "    def __init__(self, tokens=None, min_freq=1):\n",
        "        self.min_freq = min_freq\n",
        "        self.idx2token = []\n",
        "        self.token2idx = {}\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        self.build(tokens)\n",
        "\n",
        "    def build(self, tokens):\n",
        "        # tokens: iterable of characters (flattened)\n",
        "        counts = Counter(tokens)\n",
        "        # required specials\n",
        "        specials = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
        "        self.idx2token = specials + [t for t, c in counts.items() if c >= self.min_freq and t not in specials]\n",
        "        self.token2idx = {t: i for i, t in enumerate(self.idx2token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def token2index(self, t):\n",
        "        return self.token2idx.get(t, self.token2idx.get(UNK_TOKEN))\n",
        "\n",
        "    def index2token(self, i):\n",
        "        return self.idx2token[i]\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "def char_tokenize_spaced(text):\n",
        "    # returns list of characters (no spaces removed since we want spaces as token too)\n",
        "    # preserve spaces as actual ' ' tokens\n",
        "    return list(text)\n",
        "\n",
        "class TranslitDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, trg_vocab, max_len=200):\n",
        "        self.src = df[\"Urdu Text\"].tolist()\n",
        "        self.trg = df[\"Roman Transliteration\"].tolist()\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def encode_seq(self, seq, vocab):\n",
        "        # add SOS and EOS for target only outside if needed\n",
        "        ids = [vocab.token2index(ch) for ch in seq]\n",
        "        return ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_chars = char_tokenize_spaced(self.src[idx])\n",
        "        trg_chars = char_tokenize_spaced(self.trg[idx])\n",
        "        src_ids = [self.src_vocab.token2index(ch) for ch in src_chars]\n",
        "        trg_ids = [self.trg_vocab.token2index(ch) for ch in trg_chars]\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch: list of (src_ids, trg_ids)\n",
        "    src_seqs, trg_seqs = zip(*batch)\n",
        "    src_lengths = [len(s) for s in src_seqs]\n",
        "    trg_lengths = [len(t) for t in trg_seqs]\n",
        "    max_src = max(src_lengths)\n",
        "    max_trg = max(trg_lengths) + 2  # for SOS EOS in decoder input/output\n",
        "\n",
        "    padded_src = torch.full((len(batch), max_src), fill_value=src_vocab.token2index(PAD_TOKEN), dtype=torch.long)\n",
        "    padded_trg = torch.full((len(batch), max_trg), fill_value=trg_vocab.token2index(PAD_TOKEN), dtype=torch.long)\n",
        "\n",
        "    for i, (s, t) in enumerate(zip(src_seqs, trg_seqs)):\n",
        "        padded_src[i, :len(s)] = s\n",
        "        # target decoder input: <sos> + trg + <eos>\n",
        "        trg_in = [trg_vocab.token2index(SOS_TOKEN)] + t.tolist() + [trg_vocab.token2index(EOS_TOKEN)]\n",
        "        padded_trg[i, :len(trg_in)] = torch.tensor(trg_in, dtype=torch.long)\n",
        "\n",
        "    src_lengths = torch.tensor(src_lengths, dtype=torch.long)\n",
        "    trg_lengths = torch.tensor([len(t)+2 for t in trg_seqs], dtype=torch.long)\n",
        "    return padded_src.to(DEVICE), src_lengths.to(DEVICE), padded_trg.to(DEVICE), trg_lengths.to(DEVICE)\n",
        "\n",
        "# ---------- Seq2Seq Model ----------\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0.0)\n",
        "        # we'll use a linear to map encoder final states to decoder initial states\n",
        "        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: (B, L)\n",
        "        embedded = self.embedding(src)  # (B, L, E)\n",
        "        # pack padded would be better, but for brevity we won't pack; model handles paddings via embeddings PAD idx\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)  # hidden: (2*n_layers, B, hid_dim)\n",
        "        # Take last layer's forward and backward hidden (for each layer it's organized as (fwd_l1, bwd_l1, fwd_l2, bwd_l2,...))\n",
        "        # We'll extract last forward and backward pair\n",
        "        # hidden shape: (num_directions * n_layers, B, hid_dim)\n",
        "        # For simplicity, take the last two rows (last layer fwd and bwd)\n",
        "        # hidden[-2] = forward last layer, hidden[-1] = backward last layer\n",
        "        h_last_fwd = hidden[-2]  # (B, hid_dim)\n",
        "        h_last_bwd = hidden[-1]  # (B, hid_dim)\n",
        "        c_last_fwd = cell[-2]\n",
        "        c_last_bwd = cell[-1]\n",
        "\n",
        "        h_cat = torch.cat((h_last_fwd, h_last_bwd), dim=1)  # (B, hid_dim*2)\n",
        "        c_cat = torch.cat((c_last_fwd, c_last_bwd), dim=1)\n",
        "\n",
        "        h_dec = torch.tanh(self.fc_hidden(h_cat))  # (B, hid_dim)\n",
        "        c_dec = torch.tanh(self.fc_cell(c_cat))    # (B, hid_dim)\n",
        "\n",
        "        # return full outputs for attention (not used here) and decoder initial states\n",
        "        return outputs, (h_dec.unsqueeze(0), c_dec.unsqueeze(0))  # (1,B,hid) as initial for decoder\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0.0)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.n_layers = n_layers\n",
        "        self.hid_dim = hid_dim\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        # input_token: (B,) single token ids\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # (B,1,E)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (B, output_dim)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, dec_n_layers):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.dec_n_layers = dec_n_layers\n",
        "\n",
        "        # If encoder produces only 1 layer hidden (h_dec shape (1,B,H)), but decoder expects dec_n_layers,\n",
        "        # we replicate/expand encoder state to match decoder layers\n",
        "    def forward(self, src, src_lengths, trg=None, teacher_forcing_ratio=0.5):\n",
        "        # src: (B, L), trg: (B, T) with <sos>... we assume trg includes SOS as first token\n",
        "        batch_size = src.shape[0]\n",
        "        max_trg_len = trg.shape[1] if trg is not None else 200\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_trg_len, trg_vocab_size).to(DEVICE)\n",
        "        enc_outputs, (h_enc, c_enc) = self.encoder(src, src_lengths)  # h_enc: (1,B,H)\n",
        "\n",
        "        # Expand encoder hidden to decoder layers\n",
        "        # h_enc: (1,B,H) -> create (dec_layers,B,H) by repeating\n",
        "        h_init = h_enc.repeat(self.dec_n_layers, 1, 1).contiguous()\n",
        "        c_init = c_enc.repeat(self.dec_n_layers, 1, 1).contiguous()\n",
        "\n",
        "        # first input token is assumed in trg[:,0] as <sos>\n",
        "        input_tok = trg[:, 0]\n",
        "\n",
        "        hidden, cell = h_init, c_init\n",
        "\n",
        "        for t in range(1, max_trg_len):\n",
        "            output, hidden, cell = self.decoder(input_tok, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_tok = trg[:, t] if (trg is not None and teacher_force) else top1\n",
        "        return outputs\n",
        "\n",
        "# ---------- Levenshtein / CER ----------\n",
        "def levenshtein(a, b):\n",
        "    # a, b are lists or strings\n",
        "    if len(a) < len(b):\n",
        "        return levenshtein(b, a)\n",
        "    # now len(a) >= len(b)\n",
        "    previous_row = list(range(len(b) + 1))\n",
        "    for i, ca in enumerate(a, start=1):\n",
        "        current_row = [i]\n",
        "        for j, cb in enumerate(b, start=1):\n",
        "            insertions = previous_row[j] + 1\n",
        "            deletions = current_row[j-1] + 1\n",
        "            substitutions = previous_row[j-1] + (0 if ca == cb else 1)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def cer_score(ref, hyp):\n",
        "    # ref,hyp strings\n",
        "    edits = levenshtein(list(ref), list(hyp))\n",
        "    denom = max(1, len(ref))\n",
        "    return edits / denom\n",
        "\n",
        "# ---------- BLEU function ----------\n",
        "smooth = SmoothingFunction().method4\n",
        "def corpus_bleu_from_lists(references, hypotheses):\n",
        "    # references: list of reference token lists (each inner list: list of reference lists)\n",
        "    # hypotheses: list of hypothesis token lists\n",
        "    scores = []\n",
        "    for refs, hyp in zip(references, hypotheses):\n",
        "        try:\n",
        "            sc = sentence_bleu(refs, hyp, smoothing_function=smooth)\n",
        "        except Exception:\n",
        "            sc = 0.0\n",
        "        scores.append(sc)\n",
        "    return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "# ---------- Experiment runner ----------\n",
        "def build_vocabs(df_all, min_freq=1):\n",
        "    # gather chars from both columns\n",
        "    src_chars = []\n",
        "    trg_chars = []\n",
        "    for s in df_all[\"Urdu Text\"]:\n",
        "        src_chars.extend(list(s))\n",
        "    for t in df_all[\"Roman Transliteration\"]:\n",
        "        trg_chars.extend(list(t))\n",
        "    sv = CharVocab(src_chars, min_freq=min_freq)\n",
        "    tv = CharVocab(trg_chars, min_freq=min_freq)\n",
        "    # ensure specials included\n",
        "    for sp in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n",
        "        if sp not in sv.token2idx:\n",
        "            sv.idx2token.insert(0, sp)\n",
        "            sv.token2idx = {t:i for i,t in enumerate(sv.idx2token)}\n",
        "        if sp not in tv.token2idx:\n",
        "            tv.idx2token.insert(0, sp)\n",
        "            tv.token2idx = {t:i for i,t in enumerate(tv.idx2token)}\n",
        "    return sv, tv\n",
        "\n",
        "def decode_indices(indices, vocab):\n",
        "    # indices: list of ints (without PAD)\n",
        "    tokens = []\n",
        "    for i in indices:\n",
        "        t = vocab.index2token(i)\n",
        "        if t in (PAD_TOKEN, SOS_TOKEN, EOS_TOKEN):\n",
        "            continue\n",
        "        tokens.append(t)\n",
        "    return \"\".join(tokens)\n",
        "\n",
        "def greedy_decode(model, src_batch, src_lengths, max_len=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_size = src_batch.size(0)\n",
        "        # create dummy trg with SOS at position 0 for each\n",
        "        sos_idx = trg_vocab.token2index(SOS_TOKEN)\n",
        "        trg_dummy = torch.full((batch_size, max_len), fill_value=trg_vocab.token2index(PAD_TOKEN), dtype=torch.long).to(DEVICE)\n",
        "        trg_dummy[:,0] = sos_idx\n",
        "        outputs = model(src_batch, src_lengths, trg_dummy, teacher_forcing_ratio=0.0)  # (B, T, V)\n",
        "        preds = outputs.argmax(2).cpu().numpy()  # (B, T)\n",
        "    # convert preds to strings\n",
        "    results = []\n",
        "    for p in preds:\n",
        "        # stop at EOS if appears\n",
        "        chars = []\n",
        "        for idx in p:\n",
        "            tok = trg_vocab.index2token(idx)\n",
        "            if tok == EOS_TOKEN:\n",
        "                break\n",
        "            if tok == PAD_TOKEN or tok == SOS_TOKEN:\n",
        "                continue\n",
        "            chars.append(tok)\n",
        "        results.append(\"\".join(chars))\n",
        "    return results\n",
        "\n",
        "# ---------- Main: prepare data (download extraction is manual here) ----------\n",
        "# If you have dataset.zip in working dir, uncomment download_and_extract() call and adapt for your environment.\n",
        "# unzip_dir = download_and_extract()\n",
        "unzip_dir = \"urdu_dataset\"  # assume dataset already extracted into this folder. If not, extract dataset.zip manually here.\n",
        "if not os.path.exists(unzip_dir):\n",
        "    raise RuntimeError(f\"Please extract dataset.zip into folder '{unzip_dir}' (or run download_and_extract()).\")\n",
        "\n",
        "df = collect_pairs(unzip_dir)\n",
        "print(\"Initial pairs:\", len(df))\n",
        "\n",
        "# Clean\n",
        "df[\"Urdu Text\"] = df[\"Urdu Text\"].apply(clean_urdu)\n",
        "df[\"Roman Transliteration\"] = df[\"Roman Transliteration\"].apply(clean_roman)\n",
        "# drop empty after cleaning\n",
        "df = df[(df[\"Urdu Text\"].str.len() > 0) & (df[\"Roman Transliteration\"].str.len() > 0)].reset_index(drop=True)\n",
        "print(\"After cleaning pairs:\", len(df))\n",
        "\n",
        "# Save full cleaned dataset\n",
        "df.to_csv(\"all_pairs_clean.csv\", index=False)\n",
        "\n",
        "# Build vocabs\n",
        "src_vocab, trg_vocab = build_vocabs(df, min_freq=1)\n",
        "print(\"Src vocab size:\", len(src_vocab), \"Trg vocab size:\", len(trg_vocab))\n",
        "\n",
        "# Add tokens columns for convenience\n",
        "df[\"Urdu Tokens\"] = df[\"Urdu Text\"].apply(lambda x: \" \".join(list(x)))\n",
        "df[\"Roman Tokens\"] = df[\"Roman Transliteration\"].apply(lambda x: \" \".join(list(x)))\n",
        "\n",
        "# ---------- Splits: 50/25/25 ----------\n",
        "train_df, rest_df = train_test_split(df, test_size=0.5, random_state=42)\n",
        "valid_df, test_df = train_test_split(rest_df, test_size=0.5, random_state=42)\n",
        "\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "valid_df.to_csv(\"valid_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)\n",
        "print(\"Saved CSV splits: train/valid/test sizes\", len(train_df), len(valid_df), len(test_df))\n",
        "\n",
        "# ---------- Experiments config ----------\n",
        "experiments = [\n",
        "    {\n",
        "        \"name\": \"exp1\",\n",
        "        \"emb_dim\": 128,\n",
        "        \"hid_dim\": 256,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"exp2\",\n",
        "        \"emb_dim\": 256,\n",
        "        \"hid_dim\": 256,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"exp3\",\n",
        "        \"emb_dim\": 256,\n",
        "        \"hid_dim\": 512,\n",
        "        \"enc_layers\": 2,\n",
        "        \"dec_layers\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lr\": 1e-4,\n",
        "        \"batch_size\": 32\n",
        "    }\n",
        "]\n",
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "# ---------- Training / Evaluation ----------\n",
        "for cfg in experiments:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Starting experiment:\", cfg[\"name\"])\n",
        "    print(cfg)\n",
        "    # Build model\n",
        "    encoder = EncoderBiLSTM(input_dim=len(src_vocab), emb_dim=cfg[\"emb_dim\"], hid_dim=cfg[\"hid_dim\"], n_layers=cfg[\"enc_layers\"], dropout=cfg[\"dropout\"], pad_idx=src_vocab.token2index(PAD_TOKEN)).to(DEVICE)\n",
        "    decoder = DecoderLSTM(output_dim=len(trg_vocab), emb_dim=cfg[\"emb_dim\"], hid_dim=cfg[\"hid_dim\"], n_layers=cfg[\"dec_layers\"], dropout=cfg[\"dropout\"], pad_idx=trg_vocab.token2index(PAD_TOKEN)).to(DEVICE)\n",
        "    model = Seq2Seq(encoder, decoder, dec_n_layers=cfg[\"dec_layers\"]).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab.token2index(PAD_TOKEN))\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_dataset = TranslitDataset(train_df, src_vocab, trg_vocab)\n",
        "    val_dataset = TranslitDataset(valid_df, src_vocab, trg_vocab)\n",
        "    test_dataset = TranslitDataset(test_df, src_vocab, trg_vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    best_bleu = 0.0\n",
        "\n",
        "    for epoch in range(1, N_EPOCHS+1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        for src_batch, src_lens, trg_batch, trg_lens in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_batch, src_lens, trg_batch, teacher_forcing_ratio=0.5)  # (B, T, V)\n",
        "            output_dim = output.shape[-1]\n",
        "            # Flatten predictions and targets (ignore first token <sos> in target)\n",
        "            pred = output[:,1:,:].reshape(-1, output_dim)\n",
        "            target = trg_batch[:,1:].reshape(-1)\n",
        "            loss = criterion(pred, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss_sum += loss.item()\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "\n",
        "        # Validate: compute loss + generate outputs for BLEU/CER\n",
        "        model.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "        cer_vals = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src_batch, src_lens, trg_batch, trg_lens in val_loader:\n",
        "                output = model(src_batch, src_lens, trg_batch, teacher_forcing_ratio=0.0)  # no TF\n",
        "                output_dim = output.shape[-1]\n",
        "                pred = output[:,1:,:].reshape(-1, output_dim)\n",
        "                target = trg_batch[:,1:].reshape(-1)\n",
        "                loss = criterion(pred, target)\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # greedy decode for metrics\n",
        "                hyps = greedy_decode(model, src_batch, src_lens, max_len=trg_batch.size(1))\n",
        "                # build references\n",
        "                for i in range(trg_batch.size(0)):\n",
        "                    # reconstruct reference string (remove SOS/EOS/PAD)\n",
        "                    trg_ids = trg_batch[i].cpu().numpy().tolist()\n",
        "                    # remove SOS\n",
        "                    if len(trg_ids) > 0 and trg_ids[0] == trg_vocab.token2index(SOS_TOKEN):\n",
        "                        trg_ids = trg_ids[1:]\n",
        "                    # collect until EOS or PAD\n",
        "                    ref_chars = []\n",
        "                    for idx in trg_ids:\n",
        "                        tok = trg_vocab.index2token(idx)\n",
        "                        if tok == EOS_TOKEN or tok == PAD_TOKEN:\n",
        "                            break\n",
        "                        if tok == SOS_TOKEN:\n",
        "                            continue\n",
        "                        ref_chars.append(tok)\n",
        "                    references.append([ref_chars])  # sentence_bleu expects list of references\n",
        "                hypotheses.extend([list(h) for h in hyps])\n",
        "                # CER per sample\n",
        "                for ref_chars, hyp_str in zip(references[-trg_batch.size(0):], hyps):\n",
        "                    # ref_chars is [[c1,c2,...]] so pick first\n",
        "                    ref_str = \"\".join(ref_chars[0])\n",
        "                    cer_vals.append(cer_score(ref_str, hyp_str))\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        perplexity = math.exp(val_loss) if val_loss < 700 else float(\"inf\")  # avoid overflow\n",
        "\n",
        "        # Compute BLEU over the whole validation set (references/hypotheses assembled)\n",
        "        # Note: our `references` list has one element per validation sequence (list of lists)\n",
        "        # but we appended in loop accumulating; make sure lengths match\n",
        "        # If lengths mismatch, take min\n",
        "        n_items = min(len(references), len(hypotheses))\n",
        "        refs_trim = references[:n_items]\n",
        "        hyps_trim = hypotheses[:n_items]\n",
        "        # convert hyps_trim to lists of tokens\n",
        "        hyp_tokens = [list(h) for h in hyps_trim]\n",
        "        bleu = corpus_bleu_from_lists(refs_trim, hyp_tokens)\n",
        "        avg_cer = sum(cer_vals)/len(cer_vals) if cer_vals else 0.0\n",
        "\n",
        "        print(f\"[{cfg['name']}] Epoch {epoch}/{N_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | PPL: {perplexity:.3f} | BLEU: {bleu:.4f} | CER: {avg_cer:.4f}\")\n",
        "\n",
        "        # print a few qualitative examples\n",
        "        print(\"Some validation examples (src => pred | ref):\")\n",
        "        # pick first batch from val_loader for demonstration\n",
        "        for i in range(3):\n",
        "            # random sample from val dataset\n",
        "            idx = random.randint(0, len(val_dataset)-1)\n",
        "            src_str = val_dataset.src[idx]\n",
        "            ref_str = val_dataset.trg[idx]\n",
        "            # encode single example\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                s_ids = torch.tensor([ [src_vocab.token2index(ch) for ch in list(src_str)] ], dtype=torch.long).to(DEVICE)\n",
        "                s_len = torch.tensor([s_ids.size(1)], dtype=torch.long).to(DEVICE)\n",
        "                pred_list = greedy_decode(model, s_ids, s_len, max_len=200)\n",
        "            print(f\"{src_str} => {pred_list[0]} | {ref_str}\")\n",
        "\n",
        "    # After training each experiment, you can optionally save the model\n",
        "    torch.save(model.state_dict(), f\"seq2seq_{cfg['name']}.pth\")\n",
        "    print(f\"Saved model seq2seq_{cfg['name']}.pth\")\n",
        "\n",
        "print(\"All experiments complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu7xwh2ZxA-E",
        "outputId": "85e51254-b74f-421a-b2d3-48b753ed888b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Initial pairs: 21003\n",
            "After cleaning pairs: 21003\n",
            "Src vocab size: 53 Trg vocab size: 40\n",
            "Saved CSV splits: train/valid/test sizes 10501 5251 5251\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp1\n",
            "{'name': 'exp1', 'emb_dim': 128, 'hid_dim': 256, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 64}\n",
            "[exp1] Epoch 1/5 | Train Loss: 3.0083 | Val Loss: 2.9997 | PPL: 20.081 | BLEU: 0.0222 | CER: 1.0667\n",
            "Some validation examples (src => pred | ref):\n",
            "میں نے مدت سے کوئی خواب نہیں دیکہا ہے => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | maiñ ne muddat se koī ḳhvāb nahīñ dekhā hai\n",
            "پلکوں پہ کچی نیندوں کا رس پہیلتا ہو جب => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | palkoñ pe kachchī nīñdoñ kā ras phailtā ho jab\n",
            "جو بے ثبات ہو اس سر خوشی کو کیا کیجے => kaa  aaaa aaa aaaa aaaa aaaaaaa aaaaaaaaaaaaaaaaa | jo besabāt ho us sarḳhushī ko kyā kiije\n",
            "[exp1] Epoch 2/5 | Train Loss: 2.7256 | Val Loss: 2.9792 | PPL: 19.672 | BLEU: 0.0337 | CER: 0.8315\n",
            "Some validation examples (src => pred | ref):\n",
            "جو یہاں سے کہیں نہ جاتا تہا => ka   aa   aa   aa   aa   aa  aa  aa  aa  | jo yahāñ se kahīñ na jaatā thā\n",
            "دل جو ہے اگ لگا دوں اس کو => dar   aa   aa   aa   aa   aa  aa  aa  aa  | dil jo hai aag lagā duuñ us ko\n",
            "نگاہ عکس فروش و خیال ائنہ ساز => na ha   aa   aa   aa   aa  aa  aa  aa  aa | nigāh aksfarosh o ḳhayāl āinasāz\n",
            "[exp1] Epoch 3/5 | Train Loss: 2.6185 | Val Loss: 2.9291 | PPL: 18.710 | BLEU: 0.0425 | CER: 0.7656\n",
            "Some validation examples (src => pred | ref):\n",
            "جو دے سکا نہ پہاڑوں کو برف کی چادر => ja  a  a  aa  a  aa  a  aa  aa  aa  aa  aa | jo de sakā na pahāḍoñ ko barf kī chādar\n",
            "مت سہل ہمیں جانو پہرتا ہے فلک برسوں => mar  aa  a  aa  a  aa  aa  aa  aa  aa  aa  | mat sahl hameñ jaano phirtā hai falak barsoñ\n",
            "سیاہی ہے مرے ایام میں لوح دبستاں کی => saar  aa  a  aa  a  aa  aa  aa  aa  aa | siyāhī hai mire ayyām meñ lauhedabistāñ kī\n",
            "[exp1] Epoch 4/5 | Train Loss: 2.5449 | Val Loss: 2.9011 | PPL: 18.195 | BLEU: 0.0509 | CER: 0.7565\n",
            "Some validation examples (src => pred | ref):\n",
            "کہ خواب بہی مرے رخصت ہیں رتجگا بہی گیا => kahīñ ka  aa   aa  aa   aa  aa   aa  aa  aa  aa  | ki ḳhvāb bhī mire ruḳhsat haiñ ratjagā bhī gayā\n",
            "شرک چہوڑا تو سب نے چہوڑ دیا => sharā  aa  aa   aa  aa  aa  aa  aa  aa  aa  aa | shirk chhoḍā to sab ne chhoḍ diyā\n",
            "کاغذ کا یہ شہر اڑ نہ جائے => kyā ka  aa   aa  aa   aa  aa   aa  aa  aa  aa  aa | kāġhaz kā ye shahr uḍ na jaae\n",
            "[exp1] Epoch 5/5 | Train Loss: 2.4968 | Val Loss: 2.8758 | PPL: 17.739 | BLEU: 0.0657 | CER: 0.6958\n",
            "Some validation examples (src => pred | ref):\n",
            "یہ مطلع اسدؔ جوہر افسون سخن ہو => ye ka  a  a  a  a  a  a  a  a  a  a  a hai | ye matla asad jauhareafsūnesuḳhan ho\n",
            "شوخی سی ہے سوال مکرر میں اے کلیم => shaaa  a  a  a  a  a  a  a  a  a  a  aa  aa  | shoḳhī sī hai savālemukarrar meñ ai kalīm\n",
            "جا کے پہنچی ہے حد ظلمت کوں => jaat  aa  a  a  a  a  a  a  a  a  a  a  aa  aa  | jā ke pahuñchī hai haddezulmat kuuñ\n",
            "Saved model seq2seq_exp1.pth\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp2\n",
            "{'name': 'exp2', 'emb_dim': 256, 'hid_dim': 256, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.1, 'lr': 0.0005, 'batch_size': 64}\n",
            "[exp2] Epoch 1/5 | Train Loss: 3.0402 | Val Loss: 2.9852 | PPL: 19.790 | BLEU: 0.0154 | CER: 0.9726\n",
            "Some validation examples (src => pred | ref):\n",
            "گل فشانی ہائے ناز جلوہ کو کیا ہو گیا => ka                                         | gulfishānīhāenāzejalva ko kyā ho gayā\n",
            "اسدؔ یہ عجز و بے سامانیٔ فرعون توام ہے => ka                                         | asad ye ijzobesāmānīefiraunetauam hai\n",
            "دیکہیے ہوتی ہے کب راہی سوئے ملک عدم => ka                                         | dekhiye hotī hai kab raahī sūemulkeadam\n",
            "[exp2] Epoch 2/5 | Train Loss: 2.8894 | Val Loss: 2.9748 | PPL: 19.586 | BLEU: 0.0469 | CER: 0.7718\n",
            "Some validation examples (src => pred | ref):\n",
            "مجبور یاں تلک ہوئے اے اختیار حیف => mar  a   a  a  a  a  a  a  a  a ha  haa | majbūr yaañ talak hue ai iḳhtiyār haif\n",
            "ہجر ہو یا وصال ہو کچہ ہو => ha             a  a  a  a  a  aa ha haa | hijr ho yā visāl ho kuchh ho\n",
            "شب تیرہ میں شمع روشن کو => saha  a  a  a  a  a  a  a  a  a ha  haa haa | shabetīra meñ shama raushan ko\n",
            "[exp2] Epoch 3/5 | Train Loss: 2.6824 | Val Loss: 2.9555 | PPL: 19.211 | BLEU: 0.0556 | CER: 0.8310\n",
            "Some validation examples (src => pred | ref):\n",
            "یہی داغ تہے جو سجا کے ہم سر بزم یار چلے گئے => ya ha    ha    ha    ha   ha   ha   ha ha ha ha haa | yahī daaġh the jo sajā ke ham sarebazmeyār chale gae\n",
            "وقت کی ضرب سے کٹ جاتے ہیں سب کے سینے => va ha    ha    ha    ha   ha   ha  ha ha ha ha haa | vaqt kī zarb se kat jaate haiñ sab ke siine\n",
            "اگرچہ سینے میں سانس بہی ہے نہیں طبیعت میں جان باقی => as    ha   aa   aa   aa   ha   ha  ha ha ha haa | agarche siine meñ saañs bhī hai nahīñ tabīat meñ jaan baaqī\n",
            "[exp2] Epoch 4/5 | Train Loss: 2.5987 | Val Loss: 2.8772 | PPL: 17.765 | BLEU: 0.0565 | CER: 0.7842\n",
            "Some validation examples (src => pred | ref):\n",
            "اس شہر خرابی میں غم عشق کے مارے => as ka                          ha    ha   aa  aa | is shahreḳharābī meñ ġhameishq ke maare\n",
            "کبہی پلکوں پہ چمکتی ہے جو اشکوں کی لکیر => kash    aa                         ha    ha   aa  aa | kabhī palkoñ pe chamaktī hai jo ashkoñ kī lakīr\n",
            "مجہے تو روز کسوٹی پہ درد کستا ہے => mash    aa                        ha    ha   aa  aa | mujhe to roz kasautī pe dard kastā hai\n",
            "[exp2] Epoch 5/5 | Train Loss: 2.5126 | Val Loss: 2.9045 | PPL: 18.256 | BLEU: 0.0839 | CER: 0.6746\n",
            "Some validation examples (src => pred | ref):\n",
            "مگر یہ سب ہے مجہے ناگوار عید کے دن => marar ka  aa  aa  aa  aa  aa  aa  aa  aahāñ | magar ye sab hai mujhe nāgavār eid ke din\n",
            "رکہ لیجو میرے دعوی وارستگی کی شرم => rakh  aa  aa  aa  aa  aa  aa  aa  aa  aahā | rakh liijo mere dāvaevārastagī kī sharm\n",
            "رہا بلا میں بہی میں مبتلائے افت رشک => rahh  aa  aa  aa  aa  aa  aa  aa  aa  aahā | rahā balā meñ bhī maiñ mubtalaeāfaterashk\n",
            "Saved model seq2seq_exp2.pth\n",
            "\n",
            "============================================================\n",
            "Starting experiment: exp3\n",
            "{'name': 'exp3', 'emb_dim': 256, 'hid_dim': 512, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.0001, 'batch_size': 32}\n",
            "[exp3] Epoch 1/5 | Train Loss: 3.0403 | Val Loss: 2.9814 | PPL: 19.716 | BLEU: 0.0157 | CER: 0.9357\n",
            "Some validation examples (src => pred | ref):\n",
            "ہیں اہل خرد کس روش خاص پہ نازاں => ka                                       | haiñ ahleḳhirad kis ravisheḳhās pe nāzāñ\n",
            "چراغ ہم نے جلائے ہوا کے رستے میں => ka                                       | charāġh ham ne jalāe havā ke raste meñ\n",
            "خار خار الم حسرت دیدار تو ہے => ka                                       | ḳhaar ḳhārealamehasratedīdār to hai\n",
            "[exp3] Epoch 2/5 | Train Loss: 2.9182 | Val Loss: 2.9777 | PPL: 19.642 | BLEU: 0.0460 | CER: 0.7735\n",
            "Some validation examples (src => pred | ref):\n",
            "کہو گیا اج کہاں رزق کا دینے والا => ka    aa   aa   aa   aa   aa  aa  aa  aa ha | kho gayā aaj kahāñ rizq kā dene vaalā\n",
            "اپنے ماضی کی جستجو میں بہار => aa   aa   aa   aa   aa   aa  aa  aa  aa ha | apne maazī kī justujū meñ bahār\n",
            "جفا ہو یا وفا ہم سب میں خوش ہیں => mar  a  a  a  a  a  a  aa  aa  aa  aa  aa ha | jafā ho yā vafā ham sab meñ ḳhush haiñ\n",
            "[exp3] Epoch 3/5 | Train Loss: 2.7382 | Val Loss: 3.0070 | PPL: 20.226 | BLEU: 0.0602 | CER: 0.7029\n",
            "Some validation examples (src => pred | ref):\n",
            "وقت کی ضرب سے کٹ جاتے ہیں سب کے سینے => mar  a  aa   aa   aa   aa   aa   aa   hai | vaqt kī zarb se kat jaate haiñ sab ke siine\n",
            "مگر یہ شوہروں سے اپنے بے پروا نہیں ہوتیں => mar  a  aa   aa   aa   aa   aa   aa   hai | magar ye shauharoñ se apne beparvā nahīñ hotīñ\n",
            "عمر بہر ہم رہے شرابی سے => as  aa   aa   aa   aa   aa   aa   aa  hai | umr bhar ham rahe sharābī se\n",
            "[exp3] Epoch 4/5 | Train Loss: 2.6540 | Val Loss: 2.9330 | PPL: 18.784 | BLEU: 0.0436 | CER: 0.7345\n",
            "Some validation examples (src => pred | ref):\n",
            "تجہ سے دو چار ہونے کی حسرت کے مبتلا => mar   aa    aa    aa    aa    aa   aa   aa   hai | tujh se dochār hone kī hasrat ke mubtilā\n",
            "جب سر میں ہوائے طاعت تہی سرسبز شجر امید کا تہا => ji   aa    aa    aa    aa    aa   aa   aa   aa  aa | jab sar meñ havāetāat thī sarsabz shajar ummīd kā thā\n",
            "ایسے جہوٹے کو اور کیا کہئے => as ka  a    ha   aa    aa   aa   aa   aa  aa | aise jhūte ko aur kyā kahiye\n",
            "[exp3] Epoch 5/5 | Train Loss: 2.5826 | Val Loss: 2.9298 | PPL: 18.725 | BLEU: 0.0503 | CER: 0.8610\n",
            "Some validation examples (src => pred | ref):\n",
            "کعبہ کس منہ سے جاوگے غالبؔ => kai  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aaar | kaaba kis muñh se jāoge ġhālib\n",
            "کسی چراغ کے بس میں دہواں نہیں ہوتا => kis  a  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aaar hai | kisī charāġh ke bas meñ dhuāñ nahīñ hotā\n",
            "اس سے دل تباہ کی روداد کیا کہوں => as ka  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa  aa hai | us se diletabāh kī rūdād kyā kahūñ\n",
            "Saved model seq2seq_exp3.pth\n",
            "All experiments complete.\n"
          ]
        }
      ]
    }
  ]
}
